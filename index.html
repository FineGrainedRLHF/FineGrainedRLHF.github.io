<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FineGrainedRLHF">
  <meta name="keywords" content="RLHF, language model, long-form question generation, reinforcement learning from human feedback">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</title>

<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-552M76N');</script>
<!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-552M76N" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ellenmellon.github.io/">Zeqiu Wu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://yushi-hu.github.io/">Yushi Hu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://weijia-shi.netlify.app/">Weijia Shi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://webdocs.cs.ualberta.ca/~dziri/">Nouha Dziri</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.alanesuhr.com/">Alane Suhr</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://prithvirajva.com/">Prithviraj Ammanabrolu</a><sup>2</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="http://nasmith.github.io/">Noah A. Smith</a><sup>1,2</sup>
            </span>
            <span class="author-block">
              <a href="https://people.ece.uw.edu/ostendorf/">Mari Ostendorf</a><sup>1</sup>,
            </span>
              <span class="author-block">
                    <a href="https://homes.cs.washington.edu/~hannaneh/">Hannaneh Hajishirzi</a><sup>1,2</sup>,
                    </span>
            

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Allen Institute for AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.11897"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/allenai/FineGrainedRLHF"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Codebase: FineGrainedRLHF</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/Yushi-Hu/tifa/tree/main/tifa_v1.0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>QA-Feedback Dataset</span>
                  </a>
              </span>


            </div>

                          

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.jpg" alt="tifa teaser">
            <h2 class="subtitle has-text-centered">
              We propose <span style="color: red;"><b>Fine-Grained RLHF</b></span>, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density,
              providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models
              associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness).
            </h2>

      
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">What are Fine-Grained Rewards?</h2>
      <div class="content has-text-justified">
        <br>
        <p>
          Prior work in RLHF focused on collecting human preferences regarding the overall quality of language model
          (LM) outputs. However, this type of holistic feedback
          offers limited information.
          In our paper, we introduce <span style="color: red;"><b>fine-grained human feedback</b></span> (e.g.,
          identification of inaccurate sentences or irrelevant sub-sentences) as an explicit training signal.
        </p>
        <p>
          Our rewards are fine-grained in two aspects:
        </p>
        <p>
          <span style="color: red;"><b>(a) Density</b></span>: We provide a reward after each segment (e.g., a sentence)
          is
          generated, similar to OpenAI's "step-by-step process reward". We found that this approach is more informative than
          holistic
          feedback and, thus, more effective for RL.
        </p>

        <p>
          <span style="color: red;"><b>(b) Multiple reward models associated with different feedback types</b></span>:
          We employ
          multiple reward models to capture different types of feedback (e.g., factual inaccuracy, irrelevance, and
          information incompleteness).
          Interestingly, we observed that these reward models both complement and compete with each other. By adjusting
          the
          weights of the reward models, we can control the balance between the different types of feedback and <span
            style="color: red;"><b>tailor the LM for different tasks</b></span> according to specific needs.
          For instance, some users may prefer short and concise outputs, while others may seek longer and more detailed
          responses.
        </p>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or
          irrelevant outputs. Reinforcement learning from human feedback (RLHF)—where human preference judgments on LM outputs are
          transformed into a learning signal—has recently shown promise in addressing these issues. 
          However, such holistic
          feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced
          user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback
          (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce
          FINE-GRAINED RLHF, a framework that enables training and learning from reward functions that are fine-grained in two
          respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating
          multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and
          information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how
          learning with such reward functions leads to improved performance, supported by both automatic and human evaluation.
          Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models.
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Updates</h2>
      <div class="content has-text-justified">
        <p>
          <b>2023/06/05</b> Our codebase, FineGrainedRLHF, is released! Customizable for different tasks/rewards.<br>
          <b>2023/06/05</b> QA-Feedback, our long-form QA dataset with human preferences + fine-grained feedback, is released!<br>
          <b>2023/06/05</b> Paper is released!<br>
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Task 1: Detoxification</h2>
      <img src="static/images/tifa_webapproach.png">
      <div class="content has-text-justified">
        <p>
          (a) <b>Overview of how TIFA evaluates the faithfulness of a synthesized image.</b> TIFA uses a language model (LM), a question answering (QA) model, and a visual question answering (VQA) model. Given a text input, we generate several question-answer pairs with the LM and then filter them via the QA model. To evaluate the faithfulness of a synthesized image to the text input, a VQA model answers these visual questions using the image, and we check the answers for correctness.
        </p>
        <p>
          (b) <b>TIFA v1.0 benchmark.</b> While TIFA is applicable to any text prompt, to allow direct comparison across different studies, and for ease of use, we introduce the TIFA v1.0 benchmark, a repository of text inputs along with pre-generated question-answer pairs. To evaluate a text-to-image model, a user first produces the images for the text inputs in TIFA v1.0 and then performs VQA with our provided tools on generated images to compute TIFA.
        </p>
        <p>
          (c) <b>Our question-answer pair generation pipeline.</b> The whole pipeline can be executed via a single inference of GPT-3 via in-context learning. Given the text prompt, GPT-3 first extracts the elements and then generates two questions for each element. The GPT-3 output is then parsed and filtered by UnifiedQA.
        </p>

      </div>
    </div>
  </div>
</section>



<section class="section is-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hu2023tifa,
title={TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering},
author={Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith,
Noah A},
journal={arXiv preprint arXiv:2303.11897},
year={2023}
}
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
