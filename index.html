<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FineGrainedRLHF">
  <meta name="keywords" content="RLHF, language model, long-form question generation, reinforcement learning from human feedback">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</title>

<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-552M76N');</script>
<!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-552M76N" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://tifa-benchmark.github.io/">
            TIFA
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ellenmellon.github.io/">Zeqiu Wu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://yushi-hu.github.io/">Yushi Hu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://weijia-shi.netlify.app/">Weijia Shi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://webdocs.cs.ualberta.ca/~dziri/">Nouha Dziri</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.alanesuhr.com/">Alane Suhr</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://prithvirajva.com/">Prithviraj Ammanabrolu</a><sup>2</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="http://nasmith.github.io/">Noah A. Smith</a><sup>1,2</sup>
            </span>
            <span class="author-block">
              <a href="https://people.ece.uw.edu/ostendorf/">Mari Ostendorf</a><sup>1</sup>,
            </span>
              <span class="author-block">
                    <a href="https://homes.cs.washington.edu/~hannaneh/">Hannaneh Hajishirzi</a><sup>1,2</sup>,
                    </span>
            

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Allen Institute for AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.11897"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/allenai/FineGrainedRLHF"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Codebase: FineGrainedRLHF</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/Yushi-Hu/tifa/tree/main/tifa_v1.0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>QA-Feedback Dataset</span>
                  </a>
              </span>


            </div>

                          

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.jpg" alt="tifa teaser">
            <h2 class="subtitle has-text-centered">
              We propose <span style="color: red;"><b>Fine-Grained RLHF</b></span>, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density,
              providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models
              associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness).
            </h2>

      
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">What are fine-grained rewards?</h2>
      <div class="content has-text-justified">
        <br>
        <p>
          Prior works on RLHF collects human preferences on the overall quality of LM outputs. However, such holistic feedback
          conveys limited information.
          In this paper, we use <span style="color: red;"><b>fine-grained human feedback</b></span> (e.g., which sentence is
          false, which sub-sentence is irrelevant) as an explicit training signal.
        </p>
        <p>
          Our rewards are fine-grained in two aspects:
        </p>
        <p>
          <span style="color: red;"><b>(a) Density</b></span>: We provide a reward after every segment (e.g., a sentence) is
          generated. This is similar to OpenAI's "step-by-step reward". We find that it is more informative than holistic
          feedback and thus more effective for reinforcement learning.
        </p>

        <p>
          <span style="color: red;"><b>(b) Multiple reward models associated with different feedback types</b></span>: We use
          multiple reward models to capture different types of feedback (e.g., factual incorrectness, irrelevance, and
          information incompleteness).
          Interestingly, we find that the reward models are both complementary and competing against each other. By changing the
          weights of the reward models, we can control the trade-off between the different types of feedback and <span
            style="color: red;"><b>customize LM for different tasks</b></span> with specific needs.
          For example, some users may want short and concise outputs, while others may want longer and more detailed outputs.
        </p>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image generation models often fail to produce images that accurately align with the text inputs. We introduce <span style="color: red;"><b>TIFA (Text-to-image Faithfulness evaluation with question Answering)</b></span>, an automatic evaluation metric that measures the faithfulness of a generated image to its text input via <span style="color: red;"><b>visual question answering (VQA). </b></span>
          </p>
           <p>
            Specifically, given a text input, we automatically generate several question-answer pairs using a <span style="color: red;"><b>language model</b></span>. We calculate image faithfulness by checking whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for <span style="color: red;"><b>fine-grained and interpretable</b></span> evaluations of generated images.TIFA also has better correlations with human judgments than existing metrics (CLIP and SPICE). 
          </p>
          
          <p>
            Based on this approach, we introduce <span style="color: red;"><b>TIFA v1.0</b></span>, a benchmark consisting of 4K diverse text inputs and 25K questions across 12 categories (object, counting, etc.). We present a comprehensive evaluation of existing text-to-image models using TIFA v1.0 and highlight the limitations and challenges of current models. For instance, we find that current text-to-image models, despite doing well on color and material, still <span style="color: red;"><b>struggle in counting, spatial relations, and composing multiple objects</b></span>. We hope our benchmark will help carefully measure the research progress in text-to-image synthesis and provide valuable insights for further research.
          </p>
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Updates</h2>
      <div class="content has-text-justified">
        <p>
          Please email the authors if you want to submit to the leaderboard! <br>
          <b>TODO</b> A finetuned Flan-T5 that allows users to generate questions locally without relying on OpenAI API. <br>
          <b>2023/04/19</b> We released the human annotations on text-to-image faithufulness for TIFA v1.0. Check it out in our repo!<br>
          <b>2023/03/24</b> TIFA v1.0 is released! We also updated our evaluation code, VQA modules, and question generation modules. <br>
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">How does it work?</h2>
      <img src="static/images/tifa_webapproach.png">
      <div class="content has-text-justified">
        <p>
          (a) <b>Overview of how TIFA evaluates the faithfulness of a synthesized image.</b> TIFA uses a language model (LM), a question answering (QA) model, and a visual question answering (VQA) model. Given a text input, we generate several question-answer pairs with the LM and then filter them via the QA model. To evaluate the faithfulness of a synthesized image to the text input, a VQA model answers these visual questions using the image, and we check the answers for correctness.
        </p>
        <p>
          (b) <b>TIFA v1.0 benchmark.</b> While TIFA is applicable to any text prompt, to allow direct comparison across different studies, and for ease of use, we introduce the TIFA v1.0 benchmark, a repository of text inputs along with pre-generated question-answer pairs. To evaluate a text-to-image model, a user first produces the images for the text inputs in TIFA v1.0 and then performs VQA with our provided tools on generated images to compute TIFA.
        </p>
        <p>
          (c) <b>Our question-answer pair generation pipeline.</b> The whole pipeline can be executed via a single inference of GPT-3 via in-context learning. Given the text prompt, GPT-3 first extracts the elements and then generates two questions for each element. The GPT-3 output is then parsed and filtered by UnifiedQA.
        </p>

      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">TIFA v1.0 Benchmark</h2>
      <img src="static/images/tifa_benchmark_new.png">
      <div class="content has-text-justified">
        <br>
        <p>
          TIFA v1.0 benchmark contains 4,081 text inputs sampled from MSCOCO, DrawBench, PartiPrompt, and PaintSkill. Each text input is paired with questions generated by GPT-3 and filtered by UnifiedQA, resulting in 25,829 questions altogether. The text inputs contain elements from 12 categories, as illustrated in the figure. We also show the most common elements from each category. In addition, we also show some example text inputs on the sides.
        </p>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="hero-body" id="leaderboard">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Text-to-Image Model Leaderboard</h2>
      <img src="static/images/tifa_leaderboard.png"  width=80%>
      <div class="content has-text-justified">
        <p><br>
          We benchmark several text-to-image models, including AttnGAN, X-LXMERT, VQ-Diffusion, minDALL-E, and Stable Diffusion v1.1, v1.5, and v2.1. The score they get on TIFA v1.0 is shown above. The horizontal axis shows their release dates. We also mark the release date of OpenAI's DALL-E 1 model. We can see a clear trend of how text-to-image models evolve over time. There is a jump in TIFA score after DALL-E is released, from 60% to 75%.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">What are Stable Diffusion models struggling on?</h2>
      <img src="static/images/question_type.png">
      <div class="content has-text-justified">
        <p><br>
          Accuracy on each type of question in the TIFA v1.0 benchmark. The text-to-image models are Stable Diffusion v1.1, v1.5, and v2.1. We order the categories by the average score Stable Diffusion v2.1 gets on corresponding questions. For COCO captions, we also include the accuracy of the ground-truth images for reference. We can see that Stable Diffusion is <span style="color: red;"><b>struggling in shape, counting, and spatial relations.</b></span> "other" mainly contains <span style="color: red;"><b>abstract art notions</b></span>, and models are also struggling with them. Besides, we observe that generating images from real image captions in COCO is much easier than generating images from free-form text prompts.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Composing multiple objects is difficult</h2>
      <img src="static/images/compositionality.png" width=60%>
      <div class="content has-text-justified">
        <p><br>
          TIFA vs. number of entities (objects, animals, humans, food) in the text input. The accuracy starts to drop when more than 5 entities are added to the text, showing that compositionality is hard for text-to-image models. Meanwhile, TIFA scores for MSCOCO ground-truth (GT) images remain consistent.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">TIFA is more correlated with human judgments than CLIP</h2>
      <img src="static/images/tifa_compare.png" width=60%>
      <div class="content has-text-justified">
          <p><br>
            We also collect human judgments on images generated by recent text-to-image models, using TIFA v1.0 text inputs. Each annotator gives a Likert Scale of 1-5 on "Does the image match the text?". This table shows the correlation between each automatic metric and human judgments. We can see that <span style="color: red;"><b>TIFA is more accurate than prior metrics (CLIP and captions)</b></span> for evaluating text-to-image faithfulness. We hypothesize that the major challenge of these prior metrics is that they summarize the image outputs and text inputs into a single representation (embedding/caption). <span style="color: red;"><b>In contrast, TIFA exploits the power of LLMs to decompose the text input into fine-grained probes, which allows VQA to capture more nuanced aspects of the text input and generated image.</b></span>
      </div>
    </div>
  </div>
</section>

<section class="section is-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hu2023tifa,
title={TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering},
author={Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith,
Noah A},
journal={arXiv preprint arXiv:2303.11897},
year={2023}
}
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
